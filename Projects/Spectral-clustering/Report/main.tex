\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{algorithm}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}


% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{19}{2018}{1-8}{12/00}{13/00}{Jia Guo, Masaya Tsukamoto, Zihan Wang and Guangting Yu}

% Short headings should be running head and authors last names

\ShortHeadings{Spectral Clustering}{Guo, Tsukamoto, Wang and Yu}
\firstpageno{1}

\begin{document}

\title{Spectral Clustering Based on Local PCA and Applications}

\author{
\name Jia Guo \email guojia@umich.edu \\
\addr Department of Mathematics\\
University of Michigan\\
Ann Arbor, MI 48109, USA
\AND
\name Masaya Tsukamoto \email masayats@umich.edu \\
\addr Department of Mathematics\\
University of Michigan\\
Ann Arbor, MI 48109, USA
\AND
\name Zihan Wang \email wzihan@umich.edu \\
\addr Department of Climate and Space Sciences and Engineering\\
University of Michigan\\
Ann Arbor, MI 48109, USA
\AND
\name Guangting Yu \email yugtmath@umich.edu \\
\addr Department of Mathematics\\
University of Michigan\\
Ann Arbor, MI 48109, USA
}

\editor{Someone}

\maketitle

\begin{abstract}
We study a spectral clustering method based on local principal component analysis (PCA) \citep{arias2017}.
Instead of dealing with pair-wise distances between points like the previous algorithms do, this algorithm is able to resolve intersections.
Especially, the new setting requires the surfaces to be smooth.
Within this smoothness assumption, there is a mathematical theorem supporting the new algorithms.
Furthermore, we apply this algorithm to autograding arithmetic problems.
\end{abstract}

\begin{keywords}
Spectral Clustering, Local principal component analysis, Autograding
\end{keywords}

\input{intro.tex}


\section{Theoretical Analysis and Examples}
While the analysis of Algorithm 4 seems within reach, there are some complications due to the fact that points near the intersection may form a cluster of their own.
See \autoref{fig1}.
The vertical line and $\infty$-shaped line are sccessfully separated by Algorithm 4 ($K = 2$, $r = 15.0$, $\epsilon = 15.0$, $\eta = 0.3$).
But points near the intersections are fully classified as ``Blue" group. 
\begin{figure}[htbp]
\centering
\vspace{-1em}
\includegraphics[width=0.9 \textwidth]{infinity_shape.png}
\vspace{-1em}
\caption{Example of intersection cluster}
\label{fig1}
\end{figure}

Instead, the authors mainly focus on some simpler variants described in Algorithm 2 and Algorithm 3, and then give a comment on the analysis of Algorithm 4.
More theoretical results on intersecting manifolds 
refer to \cite{arias2011},\cite{chen2009},\cite{soltanolkotabi2012}.
The generative model is a natural mathematical framework for multi-manifold learning where points are sampled in the vicinity of smooth surfaces embedded in Euclidean space.
One can see in the numerical experiments that the Algorithm 4 fails to deal with phenomenon that the intersection with a sharp corner.
See \autoref{sharp_720}. This is obtained by  Algorithm 4  ($K = 3$, $r = 10.0$, $\epsilon = 10.0$, $\eta = 0.5$), and is a typical result of this algorithm, where ``2" is separated by the bottomleft sharp corner.
\begin{figure}[htbp]
\centering
\vspace{-1em}
\includegraphics[width=0.9  \textwidth]{sharp_720.png}
\vspace{-1em}
\caption{Failure at the corner}
\label{sharp_720}
\end{figure}

To clarify the characteristics of the algorithm, additional ``720"-shaped data which was written intentionally smoothly is applied below.
See \autoref{smooth_720}, which is obtained by Algorithm 4 with the parameters $K = 3$, $r = 10.0$, $\epsilon = 10.0$, $\eta = 0.5$.
Due to smoothness, ``720" is successfully separated into 3 digits.

%\begin{figure}[htbp]
%\hspace{-3em}
%\subfigure{\includegraphics[width=0.6  \textwidth]{smooth_720_1.png}}
%\hspace{-2em}
%\subfigure{\includegraphics[width=0.6  \textwidth]{smooth_720_2.png}}
%\caption{Successful results with smooth data}
%\label{smooth_720}
%\end{figure}
\begin{figure}[htbp]
\centering
\vspace{-1em}
\includegraphics[width=0.9  \textwidth]{smooth_720_1.png}
%\includegraphics[width=0.8  \textwidth]{smooth_720_2.png}
\vspace{-1em}
\caption{Successful result with smooth data}
\label{smooth_720}
\end{figure}

Precisely, $K=2$ and each surface is a connected, $C^2$ and compact submanifold without boundary and of dimension $d$ embedded in $\mathbb R^D$.
Any such surface has a positive reach, which is what we use to quantify smoothness.
The clusters are generated as follows.
Each data point $x_i$ is drawn according to
$$ x_i = s_i + z_i $$
where $s_i$ is from the uniform distribution over
$S_1\cup S_2$ and $z_i$
is an additive noise term satisfying $||z_i||\le \tau$, where \(S_1\) and \(S_2\) represent two different clusters, and \(\tau\geq0\) is a non-negative constant.
The main theorem of the paper is given below.
\begin{theorem}
Consider two connected, compact, twice continuously differentiable submanifolds
without boundary, of same dimension \(d\), intersecting at a strictly positive angle, with the intersection set having strictly positive reach.
Assume the parameters are set so that
$$
\tau\le \frac{r\eta}{C},\ \ r\le\frac{\epsilon}{C},\ \ \epsilon\le\frac{\eta}{C},\ \ \eta\le\frac{1}{C}
$$
for large constant $C\ge 1$.
Then with probability at least $1-Cn\exp(- nr^d\eta^2/C)$:
\begin{itemize}
\item Algorithm 2 returns exactly two groups such that two points from different clusters are not grouped together unless one of them is within distance \(C\cdot r\) from the intersection.
\item Algorithm 3 returns at least two groups, and such that two points from different clusters are not grouped together unless one of them is within distance \(C\cdot r\) from the intersection.
\end{itemize}
\end{theorem}

\begin{proof}
We give a sketch of proof of algorithm 2 and refers to the paper for more details.
The most important notation is $I^*$, which indexes the points whose neighborhoods do not contain points from the other cluster.
% $$
% I^*=\{i: K_j=K_i,\ \forall j\in\Xi_i  \},\ \ \ \Xi_i=\{j\ne i, s_j\in N_r(s_i)   \}
% $$

\begin{enumerate}
\item Assume $\tau=0$, and then explain for how things change for $\tau>0$. Deriving a concentration inequality for local covariances with large constant $C$.
\item Then show that among points away from the intersection, those that are in the same cluster are neighbors in the graph if they are within distance $\epsilon$, while those in different clusters cannot be neighbors in the graph.
\item Confirming that step 2 in algorithm 2 can eliminate all points which are not included in $I^*$, and  the points removed are within distance \(C\cdot r\) of the intersection, furthermore,  points removed are within distance \(C\cdot r\) of the intersection.
\item Concluding that in step 4, algorithm 2 returns two graphs, and each group covers an entire cluster except for points within distance $O(r)$ of the intersection.
\end{enumerate}
\end{proof}

To conclude, the key point of the approach is using principal component analysis to study the local behavior of data, and then use the information about affinity to generate a new local neighborhood graph.
However, the selection of the parameters is quite sensitive to the final output, it is still a quite acceptable approach in many applications and perform well in particular example.

For comparison, results with different parameters are described in \autoref{fig4}, \autoref{fig5}, and \autoref{fig6}. If not explicitly mentioned, parameters $K = 3$, $r = 10.0$, $\epsilon = 10.0$, $\eta = 0.5$ are used.

\begin{figure}[htbp]
\centering
\vspace{-1em}
\hspace{-2em}
\includegraphics[width=0.33  \textwidth]{eta_small.png}
\hspace{-1em}
\includegraphics[width=0.33  \textwidth]{normal_720.png}
\hspace{-1em}
\includegraphics[width=0.33  \textwidth]{eta_large.png}
\hspace{-2em}
\vspace{-1em}
\caption{[Left]: $\eta = 0.1$,  [Middle]: $\eta = 0.5$,  [Right]: $\eta = 1.0$}
\label{fig4}
\end{figure}
\begin{figure}[htbp]
\centering
\vspace{-2em}
\hspace{-2em}
\includegraphics[width=0.33  \textwidth]{r_small.png}
\hspace{-1em}
\includegraphics[width=0.33  \textwidth]{normal_720.png}
\hspace{-1em}
\includegraphics[width=0.33  \textwidth]{r_large.png}
\hspace{-2em}
\vspace{-1em}
\caption{[Left]: $r = 3.0$,  [Middle]: $r = 10.0$,  [Right]: $r = 40.0$}
\label{fig5}
\end{figure}
\begin{figure}[htbp]
\centering
\vspace{-2em}
\hspace{-2em}
\includegraphics[width=0.33  \textwidth]{eps_small.png}
\hspace{-1em}
\includegraphics[width=0.33  \textwidth]{normal_720.png}
\hspace{-1em}
\includegraphics[width=0.33  \textwidth]{eps_large.png}
\hspace{-2em}
\vspace{-1em}
\caption{[Left]: $\epsilon = 3.0$,  [Middle]: $\epsilon = 10.0$,  [Right]: $\epsilon = 40.0$}
\label{fig6}
\end{figure}

When $\eta$ is small, local directions of data tend to have a more important role in clustering than geometric closeness between data.
In fact, in \autoref{fig4} [Left], only horizontal parts of ``7" and ``2" are labeled as green.
On the other hand, in \autoref{fig4} [Right], the orthogonal intersection doesn't work as a separating point between the vertical and the horizontal, and then a part of ``2" is absorbed in ``0".

$r$ should be chosen so that small circles with radius $r$ can appropriately cover the thickness of lines or surfaces of data points.
If $r$ is too small, the small circles may be buried in lines or surfaces.
If $r$ is too large, some circle may totally wrap around an intersection.
In both cases, the classification will fail as described in \autoref{fig5} [Left] and [Right].
\autoref{fig7} shows an enlarged view of the boundary between the blue and green areas in \autoref{fig5} [Left].
A small green circle buried in the line can be seen here.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.22  \textwidth]{boundary.png}
\vspace{-1em}
\caption{Small green circle buried in the line}
\label{fig7}
%\vspace{-1.0em}
\end{figure}

When $\epsilon$ is small, the affinity based on geometric distance decays immediately.
Therefore it is natural that \autoref{fig6} [Left] has a mottled pattern.
Conversely, when $\epsilon$ is large, the affinity remains higher over a wide distance.

\input{application.tex}

\section{Conclusion}
In this project, we study new algorithms based on local PCA.
The main contribution behind this paper is that dealing with the situations where the surfaces intersect and a strong mathematical theorem guarantees the algorithms with high probability to succeed.
However, smoothness assumption is crucial to the algorithm and does not always hold in applications.
Another drawback behind the algorithm is that the points near the intersections may generate a new cluster of their own.
But it is still a competitive method in spectral clustering, and we give an application in the real world to demonstrate the ability to handle the intersections.

                

% Acknowledgements should go at the end, before appendices and references

\acks{
\textbf{Jia Guo} reads the proofs of theorems in the paper and conduct mathematical analysis on our implementation.
\textbf{Masaya Tsukamoto} write the code of the algorithm and designed some testcases, including hand-written digit generation and smooth intersecting manifolds for separation.
\textbf{Zihan Wang} writes the introduction and additional algorithm in the paper we study.
\textbf{Guangting Yu} implements the arithmetic autograding based on the algorithm and compare with other existing algorithms.
Every group member writes the report together and contributes to the project equally.
}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix

\vskip 0.2in
% \bibliography{sample}
% \bibliographystyle{apalike}
\bibliography{ml}
\end{document}